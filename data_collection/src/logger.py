#!/usr/bin/env python3
"""
æ—¥å¿—ç®¡ç†æ¨¡å—
ç»Ÿä¸€ç®¡ç†ETFæ•°æ®ç³»ç»Ÿçš„æ‰€æœ‰æ—¥å¿—è®°å½•
"""

import os
import logging
from datetime import datetime
from typing import Optional
import json
from .smart_report_generator import SmartReportGenerator


class ETFLogger:
    """ETFç³»ç»Ÿæ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str = None):
        """åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨"""
        if base_dir is None:
            # è‡ªåŠ¨å®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
            current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            self.base_dir = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
        else:
            self.base_dir = base_dir
            
        self.logs_dir = os.path.join(self.base_dir, "logs")
        self.loggers = {}  # ç¼“å­˜ä¸åŒç±»å‹çš„logger
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        self._ensure_log_directories()
    
    def _ensure_log_directories(self):
        """ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨"""
        log_subdirs = ["etf_operations", "factor_calculations", "system", "archives"]
        for subdir in log_subdirs:
            dir_path = os.path.join(self.logs_dir, subdir)
            os.makedirs(dir_path, exist_ok=True)
    
    def get_logger(self, logger_type: str, etf_code: Optional[str] = None) -> logging.Logger:
        """è·å–æŒ‡å®šç±»å‹çš„logger"""
        logger_name = f"{logger_type}_{etf_code}" if etf_code else logger_type
        
        if logger_name in self.loggers:
            return self.loggers[logger_name]
        
        # åˆ›å»ºæ–°çš„logger
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.INFO)
        
        # é¿å…é‡å¤æ·»åŠ handler
        if not logger.handlers:
            # æ–‡ä»¶å¤„ç†å™¨
            log_file = self._get_log_file_path(logger_type, etf_code)
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            # æ—¥å¿—æ ¼å¼
            formatter = logging.Formatter(
                '%(asctime)s | %(levelname)-8s | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
            
            # æ§åˆ¶å°å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
            if logger_type == "system":
                console_handler = logging.StreamHandler()
                console_handler.setLevel(logging.WARNING)  # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
                console_handler.setFormatter(formatter)
                logger.addHandler(console_handler)
        
        self.loggers[logger_name] = logger
        return logger
    
    def _get_log_file_path(self, logger_type: str, etf_code: Optional[str] = None) -> str:
        """è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
        today = datetime.now().strftime("%Y%m%d")
        
        if logger_type == "etf_operations":
            if etf_code:
                filename = f"etf_{etf_code}_{today}.log"
            else:
                filename = f"etf_operations_{today}.log"
            return os.path.join(self.logs_dir, "etf_operations", filename)
        
        elif logger_type == "factor_calculations":
            if etf_code:
                filename = f"factors_{etf_code}_{today}.log"
            else:
                filename = f"factor_calculations_{today}.log"
            return os.path.join(self.logs_dir, "factor_calculations", filename)
        
        elif logger_type == "system":
            filename = f"system_{today}.log"
            return os.path.join(self.logs_dir, "system", filename)
        
        else:
            # é»˜è®¤ç³»ç»Ÿæ—¥å¿—
            filename = f"{logger_type}_{today}.log"
            return os.path.join(self.logs_dir, "system", filename)
    
    def log_operation(self, operation: str, etf_code: str, status: str, details: dict = None):
        """è®°å½•ETFæ“ä½œæ—¥å¿—"""
        logger = self.get_logger("etf_operations", etf_code)
        
        log_data = {
            "operation": operation,
            "etf_code": etf_code,
            "status": status,
            "timestamp": datetime.now().isoformat()
        }
        
        if details:
            log_data.update(details)
        
        # æ ¼å¼åŒ–æ—¥å¿—æ¶ˆæ¯
        message = f"{operation} | {etf_code} | {status}"
        if details:
            message += f" | {json.dumps(details, ensure_ascii=False)}"
        
        # åˆ¤æ–­æ—¥å¿—çº§åˆ«
        status_lower = status.lower()
        if any(word in status_lower for word in ["success", "completed", "æˆåŠŸ", "å®Œæˆ", "âœ…"]):
            logger.info(message)
        elif any(word in status_lower for word in ["warning", "warn", "è­¦å‘Š", "âš ï¸"]):
            logger.warning(message)
        elif any(word in status_lower for word in ["error", "failed", "å¤±è´¥", "é”™è¯¯", "âŒ"]):
            logger.error(message)
        else:
            # é»˜è®¤ä¸ºinfoçº§åˆ«
            logger.info(message)
    
    def log_factor_calculation(self, etf_code: str, factor_name: str, status: str, 
                             duration: float = None, record_count: int = None):
        """è®°å½•å› å­è®¡ç®—æ—¥å¿—"""
        logger = self.get_logger("factor_calculations", etf_code)
        
        details = {}
        if duration is not None:
            details["duration_seconds"] = round(duration, 2)
        if record_count is not None:
            details["record_count"] = record_count
        
        message = f"Factor Calculation | {etf_code} | {factor_name} | {status}"
        if details:
            message += f" | {json.dumps(details)}"
        
        # åˆ¤æ–­æ—¥å¿—çº§åˆ«
        status_lower = status.lower()
        if any(word in status_lower for word in ["success", "completed", "æˆåŠŸ", "å®Œæˆ", "âœ…"]):
            logger.info(message)
        elif any(word in status_lower for word in ["warning", "warn", "è­¦å‘Š", "âš ï¸"]):
            logger.warning(message)
        elif any(word in status_lower for word in ["error", "failed", "å¤±è´¥", "é”™è¯¯", "âŒ"]):
            logger.error(message)
        else:
            # é»˜è®¤ä¸ºinfoçº§åˆ«
            logger.info(message)
    
    def log_system_event(self, event_type: str, message: str, level: str = "info"):
        """è®°å½•ç³»ç»Ÿäº‹ä»¶æ—¥å¿—"""
        logger = self.get_logger("system")
        
        log_message = f"{event_type} | {message}"
        
        if level.lower() == "info":
            logger.info(log_message)
        elif level.lower() == "warning":
            logger.warning(log_message)
        elif level.lower() == "error":
            logger.error(log_message)
        else:
            logger.info(log_message)
    
    def get_recent_logs(self, logger_type: str, etf_code: str = None, lines: int = 50) -> list:
        """è·å–æœ€è¿‘çš„æ—¥å¿—è®°å½•"""
        log_file = self._get_log_file_path(logger_type, etf_code)
        
        if not os.path.exists(log_file):
            return []
        
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()
                return all_lines[-lines:] if len(all_lines) > lines else all_lines
        except Exception:
            return []
    
    def clean_weekly_logs(self, keep_weeks: int = 2):
        """æ¯å‘¨æ¸…ç†æ—¥å¿— - ä¿ç•™æŒ‡å®šå‘¨æ•°çš„æ—¥å¿—"""
        from datetime import timedelta
        import shutil
        import zipfile
        
        print(f"ğŸ§¹ å¼€å§‹æ¯å‘¨æ—¥å¿—æ¸…ç†...")
        
        current_time = datetime.now()
        cutoff_date = current_time - timedelta(weeks=keep_weeks)
        
        cleaned_count = 0
        archived_count = 0
        archived_size = 0
        
        log_dirs = ["etf_operations", "factor_calculations", "system"]
        
        # åˆ›å»ºå‹ç¼©å½’æ¡£
        archive_filename = f"logs_archive_{current_time.strftime('%Y%m%d_%H%M%S')}.zip"
        archive_path = os.path.join(self.logs_dir, "archives", archive_filename)
        
        # ç”¨äºå‹ç¼©çš„ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
        files_to_archive = []
        
        for log_dir in log_dirs:
            dir_path = os.path.join(self.logs_dir, log_dir)
            if not os.path.exists(dir_path):
                continue
            
            for filename in os.listdir(dir_path):
                if not filename.endswith('.log'):
                    continue
                
                file_path = os.path.join(dir_path, filename)
                file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                file_size = os.path.getsize(file_path)
                
                if file_mtime < cutoff_date:
                    # è®°å½•è¦å½’æ¡£çš„æ–‡ä»¶
                    files_to_archive.append({
                        'file_path': file_path,
                        'archive_name': f"{log_dir}/{filename}",
                        'size': file_size
                    })
                    archived_size += file_size
                    archived_count += 1
        
        # åˆ›å»ºå‹ç¼©å½’æ¡£
        if files_to_archive:
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_info in files_to_archive:
                    zipf.write(file_info['file_path'], file_info['archive_name'])
                    print(f"  ğŸ“¦ å½’æ¡£: {file_info['archive_name']} ({file_info['size']:,} bytes)")
            
            # åˆ é™¤åŸæ–‡ä»¶
            for file_info in files_to_archive:
                os.remove(file_info['file_path'])
                cleaned_count += 1
            
            compression_ratio = os.path.getsize(archive_path) / archived_size if archived_size > 0 else 0
            
            print(f"âœ… æ—¥å¿—æ¸…ç†å®Œæˆ:")
            print(f"   ğŸ“¦ å½’æ¡£æ–‡ä»¶: {archive_filename}")
            print(f"   ğŸ—‚ï¸  æ¸…ç†æ–‡ä»¶æ•°: {cleaned_count}")
            print(f"   ğŸ’¾ åŸå§‹å¤§å°: {archived_size/1024:.1f} KB")
            print(f"   ğŸ“‹ å‹ç¼©å: {os.path.getsize(archive_path)/1024:.1f} KB")
            print(f"   ğŸ“ˆ å‹ç¼©ç‡: {compression_ratio:.1%}")
            
            # è®°å½•æ¸…ç†äº‹ä»¶
            self.log_system_event("WEEKLY_CLEANUP", 
                                f"æ¸…ç†äº† {cleaned_count} ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œå½’æ¡£åˆ° {archive_filename}", 
                                "info")
        else:
            print("âœ… æ— éœ€æ¸…ç†çš„æ—¥å¿—æ–‡ä»¶")
        
        return cleaned_count, archive_path if files_to_archive else None

    def auto_cleanup_on_startup(self):
        """ç³»ç»Ÿå¯åŠ¨æ—¶è‡ªåŠ¨æ£€æŸ¥å¹¶æ¸…ç†æ—¥å¿—"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
            cleanup_marker = os.path.join(self.logs_dir, ".last_cleanup")
            current_time = datetime.now()
            
            should_cleanup = True
            if os.path.exists(cleanup_marker):
                try:
                    with open(cleanup_marker, 'r') as f:
                        last_cleanup = datetime.fromisoformat(f.read().strip())
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»è¿‡äº†ä¸€å‘¨
                    days_since_cleanup = (current_time - last_cleanup).days
                    if days_since_cleanup < 7:
                        should_cleanup = False
                        print(f"ğŸ“… è·ç¦»ä¸Šæ¬¡æ¸…ç† {days_since_cleanup} å¤©ï¼Œæ— éœ€æ¸…ç†")
                except:
                    should_cleanup = True
            
            if should_cleanup:
                print("ğŸ• æ£€æµ‹åˆ°éœ€è¦è¿›è¡Œæ¯å‘¨æ—¥å¿—æ¸…ç†...")
                cleaned_count, archive_path = self.clean_weekly_logs()
                
                # æ›´æ–°æ¸…ç†æ ‡è®°
                with open(cleanup_marker, 'w') as f:
                    f.write(current_time.isoformat())
                
                return cleaned_count > 0
            
            return False
            
        except Exception as e:
            print(f"âš ï¸  è‡ªåŠ¨æ¸…ç†å¤±è´¥: {e}")
            self.log_system_event("CLEANUP_ERROR", f"è‡ªåŠ¨æ¸…ç†å¤±è´¥: {e}", "warning")
            return False

    def archive_old_logs(self, days_old: int = 30):
        """å½’æ¡£æ—§æ—¥å¿—æ–‡ä»¶ - ä¿ç•™åŸæœ‰åŠŸèƒ½"""
        from datetime import timedelta
        import shutil
        
        cutoff_date = datetime.now() - timedelta(days=days_old)
        archived_count = 0
        
        log_dirs = ["etf_operations", "factor_calculations", "system"]
        
        for log_dir in log_dirs:
            dir_path = os.path.join(self.logs_dir, log_dir)
            if not os.path.exists(dir_path):
                continue
            
            for filename in os.listdir(dir_path):
                if not filename.endswith('.log'):
                    continue
                
                file_path = os.path.join(dir_path, filename)
                file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                if file_mtime < cutoff_date:
                    # ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•
                    archive_path = os.path.join(self.logs_dir, "archives", filename)
                    shutil.move(file_path, archive_path)
                    archived_count += 1
        
        if archived_count > 0:
            self.log_system_event("ARCHIVE", f"Archived {archived_count} old log files")
        
        return archived_count
    
    def get_log_summary(self) -> dict:
        """è·å–æ—¥å¿—ç³»ç»Ÿæ‘˜è¦"""
        summary = {
            "log_directories": [],
            "total_files": 0,
            "total_size_mb": 0
        }
        
        log_dirs = ["etf_operations", "factor_calculations", "system", "archives"]
        
        for log_dir in log_dirs:
            dir_path = os.path.join(self.logs_dir, log_dir)
            if os.path.exists(dir_path):
                files = [f for f in os.listdir(dir_path) if f.endswith('.log')]
                dir_size = sum(os.path.getsize(os.path.join(dir_path, f)) for f in files)
                
                summary["log_directories"].append({
                    "name": log_dir,
                    "file_count": len(files),
                    "size_mb": round(dir_size / 1024 / 1024, 2)
                })
                
                summary["total_files"] += len(files)
                summary["total_size_mb"] += dir_size / 1024 / 1024
        
        summary["total_size_mb"] = round(summary["total_size_mb"], 2)
        return summary
    
    def generate_smart_report(self) -> str:
        """ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Šå¹¶æ¸…ç†æ—§æ—¥å¿—"""
        try:
            generator = SmartReportGenerator(self.logs_dir)
            return generator.generate_report()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ™ºèƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            # åˆ›å»ºç®€å•çš„é”™è¯¯æŠ¥å‘Š
            error_report = f"""
=================================================================
ğŸš€ ETFæ•°æ®ç®¡ç†ç³»ç»Ÿ - æ¯æ—¥è¿è¡ŒæŠ¥å‘Š (é”™è¯¯ç‰ˆæœ¬)
=================================================================
ğŸ“… æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}
âŒ çŠ¶æ€: æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}
ğŸ“ æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
=================================================================
"""
            try:
                report_file = os.path.join(self.logs_dir, "report.txt")
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(error_report)
            except:
                pass
            return error_report


# å…¨å±€æ—¥å¿—ç®¡ç†å™¨å®ä¾‹
_global_logger = None

def get_etf_logger() -> ETFLogger:
    """è·å–å…¨å±€æ—¥å¿—ç®¡ç†å™¨å®ä¾‹"""
    global _global_logger
    if _global_logger is None:
        _global_logger = ETFLogger()
    return _global_logger