"""
CacheManager - ç¼“å­˜ç®¡ç†å™¨
æ”¯æŒå†…å­˜+ç£ç›˜ç¼“å­˜ï¼Œå¢é‡æ›´æ–°
æ–‡ä»¶é™åˆ¶: <200è¡Œ
"""

import os
import pickle
import json
import pandas as pd
from datetime import datetime
from typing import Optional, Dict, Any
from .config import config


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - æ”¯æŒå¢é‡æ›´æ–°"""
    
    def __init__(self, cache_dir: str = "factor_data/cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.cache_dir = cache_dir
        self.memory_cache = {}  # å†…å­˜ç¼“å­˜
        self.cache_file = os.path.join(cache_dir, "factor_cache.pkl")
        self.metadata_file = os.path.join(cache_dir, "metadata.json")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½ç£ç›˜ç¼“å­˜
        self._load_cache_from_disk()
        
    def get_cached_factor(self, cache_key: str) -> Optional[pd.DataFrame]:
        """
        è·å–ç¼“å­˜çš„å› å­
        Args:
            cache_key: ç¼“å­˜é”®
        Returns:
            ç¼“å­˜çš„DataFrameæˆ–None
        """
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key].copy()
        return None
    
    def cache_factor(self, cache_key: str, result: pd.DataFrame, factor_name: str = ""):
        """
        ç¼“å­˜å› å­ç»“æœ
        Args:
            cache_key: ç¼“å­˜é”®
            result: è®¡ç®—ç»“æœ
            factor_name: å› å­åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        if result.empty:
            return
            
        # å†…å­˜ç¼“å­˜
        self.memory_cache[cache_key] = result.copy()
        
        # æ›´æ–°å…ƒæ•°æ®
        self._update_metadata(cache_key, factor_name)
        
        print(f"ğŸ”„ ç¼“å­˜å› å­: {factor_name} -> {cache_key[:8]}...")
    
    def is_cached(self, cache_key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜"""
        return cache_key in self.memory_cache
    
    def save_cache_to_disk(self):
        """ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.memory_cache, f)
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜åˆ°ç£ç›˜: {self.cache_file}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _load_cache_from_disk(self):
        """ä»ç£ç›˜åŠ è½½ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    self.memory_cache = pickle.load(f)
                print(f"ğŸ“‚ ä»ç£ç›˜åŠ è½½ç¼“å­˜: {len(self.memory_cache)} é¡¹")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                self.memory_cache = {}
    
    def _update_metadata(self, cache_key: str, factor_name: str = ""):
        """æ›´æ–°å…ƒæ•°æ®"""
        metadata = self._load_metadata()
        
        metadata['factors'][cache_key] = {
            'factor_name': factor_name,
            'cached_at': datetime.now().isoformat(),
            'data_rows': len(self.memory_cache[cache_key]) if cache_key in self.memory_cache else 0
        }
        metadata['last_updated'] = datetime.now().isoformat()
        
        self._save_metadata(metadata)
    
    def _load_metadata(self) -> dict:
        """åŠ è½½å…ƒæ•°æ®"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        # é»˜è®¤å…ƒæ•°æ®ç»“æ„
        return {
            'created_at': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'factors': {}
        }
    
    def _save_metadata(self, metadata: dict):
        """ä¿å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        metadata = self._load_metadata()
        
        memory_size = sum(
            df.memory_usage(deep=True).sum() 
            for df in self.memory_cache.values()
        ) / (1024 * 1024)  # MB
        
        return {
            'memory_cached_factors': len(self.memory_cache),
            'memory_size_mb': round(memory_size, 2),
            'disk_cache_exists': os.path.exists(self.cache_file),
            'metadata': metadata
        }
    
    def clear_cache(self, confirm: bool = False):
        """
        æ¸…ç†æ‰€æœ‰ç¼“å­˜
        Args:
            confirm: ç¡®è®¤æ¸…ç†
        """
        if not confirm:
            print("âš ï¸  è¯·è®¾ç½® confirm=True ç¡®è®¤æ¸…ç†æ“ä½œ")
            return
            
        # æ¸…ç†å†…å­˜ç¼“å­˜
        self.memory_cache.clear()
        
        # åˆ é™¤ç£ç›˜ç¼“å­˜
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
            
        # é‡ç½®å…ƒæ•°æ®
        metadata = {
            'created_at': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'factors': {}
        }
        self._save_metadata(metadata)
        
        print("ğŸ—‘ï¸  æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†")
    
    def get_cache_keys_by_factor(self, factor_name: str) -> list:
        """
        æ ¹æ®å› å­åè·å–ç¼“å­˜é”®
        Args:
            factor_name: å› å­åç§°
        Returns:
            ç›¸å…³çš„ç¼“å­˜é”®åˆ—è¡¨
        """
        keys = []
        metadata = self._load_metadata()
        
        for cache_key, info in metadata.get('factors', {}).items():
            if info.get('factor_name') == factor_name:
                keys.append(cache_key)
                
        return keys
    
    def update_incremental(self, cache_key: str, new_data: pd.DataFrame, factor_name: str = ""):
        """
        å¢é‡æ›´æ–°ç¼“å­˜
        Args:
            cache_key: ç¼“å­˜é”®
            new_data: æ–°æ•°æ®
            factor_name: å› å­åç§°
        """
        if cache_key in self.memory_cache:
            # åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰ç¼“å­˜
            existing_data = self.memory_cache[cache_key]
            
            # åŸºäºtrade_dateåˆå¹¶ï¼Œé¿å…é‡å¤
            combined = pd.concat([existing_data, new_data]).drop_duplicates(
                subset=['trade_date'], keep='last'
            ).sort_values('trade_date').reset_index(drop=True)
            
            self.memory_cache[cache_key] = combined
            print(f"ğŸ“ˆ å¢é‡æ›´æ–°ç¼“å­˜: {factor_name} (+{len(new_data)} è¡Œ)")
        else:
            # æ–°ç¼“å­˜
            self.cache_factor(cache_key, new_data, factor_name)
    
    def __del__(self):
        """ææ„å‡½æ•° - è‡ªåŠ¨ä¿å­˜ç¼“å­˜"""
        if hasattr(self, 'memory_cache') and self.memory_cache:
            self.save_cache_to_disk()